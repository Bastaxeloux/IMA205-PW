{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vDA_obreM3PN"
   },
   "source": [
    "# TP n°4 : Skin lesion classification\n",
    "\n",
    "### Le Guillouzic Maël"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cJ55RYNTM3PP"
   },
   "source": [
    "**Deadline**: Upload this notebook (rename it as 'TP-SVM-YOUR-SURNAME.ipynb') to Ecampus/Moodle before the deadline.\n",
    "Complete the code where you see XXXXXXXXXXXXXXXXX (mandatory for everybody)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tBl0eH4rgj3W"
   },
   "source": [
    "**Context**\n",
    "A skin lesion is defined as a superficial growth or patch of the skin that is visually different and/or has a different texture than its surrounding area. Skin lesions, such as moles or birthmarks, can degenerate and become melanoma, one of the deadliest skin cancer. Its incidence has been increasing during the last decades, especially in the areas mostly populated by white people.\n",
    "\n",
    "The most effective treatment is an early detection followed by surgical excision. This is why several approaches for melanoma detection have been proposed in the last years (non-invasive computer-aided diagnosis (CAD) ).\n",
    "\n",
    "\n",
    "**Goal**\n",
    "The goal of this practical session is to classify images of skin lesions as either benign or melanoma using machine learning algorithms. In order to do that, you will have at your disposal a set of 30 features already extracted from 600 dermoscopic images (both normal skin lesions and melanoma from the ISIC database - https://isic-archive.com/). These features characterize the Asymmetry, the Border irregularity, the Colour and the Dimension of the lesion (the so-called ABCD rule).\n",
    "\n",
    "The features are:\n",
    "- shape asimmetry (f0 and f1)\n",
    "- difference in colors between center and periphery of the image (f2, f3, f4, f27, f28, f29)\n",
    "- geometry (f5, f6, f7)\n",
    "- other features related to eccentricity,entropy, mean, standard deviation and maximum value of each channel in RGB and HSV (f8,...,f24)\n",
    "- asimmetry of color intensity (f25, f26)\n",
    "\n",
    "Features are computed using *manually checked segmentations* and following *Ganster et al. 'Automated melanoma recognition', IEEE TMI, 2001* and *Zortea et al. 'Performance of a dermoscopy-based computer vision system for the diagnosis of pigmented skin lesions compared with visual evaluation by experienced dermatologists', Artificial Intelligence in Medicine, 2014*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gP6UCcieM3PT"
   },
   "source": [
    "First load all necessary packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "id": "ggIRZ9_UM3PU"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from skimage.io import imread\n",
    "from time import time\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.axes_grid1 import AxesGrid\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import  cross_val_score\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC, LinearSVC\n",
    "from sklearn.model_selection import GridSearchCV, KFold, cross_validate\n",
    "from sklearn.linear_model import Perceptron\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "\n",
    "from sklearn.exceptions import ConvergenceWarning\n",
    "warnings.filterwarnings(action='ignore', category=ConvergenceWarning)\n",
    "\n",
    "# try:\n",
    "#   import google.colab\n",
    "#   IN_COLAB = True\n",
    "#   print('You are using Google Colab')\n",
    "#   !pip install googledrivedownloader\n",
    "#   from googledrivedownloader import download_file_from_google_drive\n",
    "# except:\n",
    "#   IN_COLAB = False\n",
    "\n",
    "# Code from scikit-learn\n",
    "import itertools\n",
    "def plot_confusion_matrix(cm, classes,\n",
    "                          normalize=False,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=plt.cm.Blues):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \"\"\"\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        print(\"Normalized confusion matrix\")\n",
    "    else:\n",
    "        print('Confusion matrix, without normalization')\n",
    "\n",
    "    print(cm)\n",
    "\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    fmt = '.2f' if normalize else 'd'\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, format(cm[i, j], fmt),\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"black\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "    plt.tight_layout()\n",
    "\n",
    "\n",
    "from sklearn.pipeline import make_pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vFjZc3lXvxLD"
   },
   "source": [
    "Then load the data from my Google Drive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "id": "QcXyy29Qvzcb"
   },
   "outputs": [],
   "source": [
    "# if IN_COLAB:\n",
    "#   download_file_from_google_drive(file_id='18hrQVGBCfW7SKTnzmWUONo8iowBsi1DL',\n",
    "#   dest_path='./data/features.csv')\n",
    "#   download_file_from_google_drive(file_id='1iQZdUiuK_FwZ7mik7LB3eN_H_IUc5l7b',\n",
    "#   dest_path='./data/im/nevus-seg.jpg')\n",
    "#   download_file_from_google_drive(file_id='1_TeYzLLDoKbPX4xXAOAM_mQiT2nLHgvp',\n",
    "#   dest_path='./data/im/nevus.jpg')\n",
    "#   download_file_from_google_drive(file_id='1B2Ol92mBcHN6ah3bpoucBbBbHkPMGC8D',\n",
    "#   dest_path='./data/im/melanoma-seg.jpg')\n",
    "#   download_file_from_google_drive(file_id='1yZ46UzGhwO7g5T8397JpewBl6UqgRo5J',\n",
    "#   dest_path='./data/im/melanoma.jpg')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1jkc3oIG3skR"
   },
   "source": [
    "Or from yout local computer. Please download the 'data' folder in the same folder as your notebook and do not modifiy it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "id": "8F5-rVsTM3PY"
   },
   "source": [
    "Then read the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "id": "KszrmyDJM3PZ"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "## Read data\n",
    "Working_directory=\"./data/\"\n",
    "df = pd.read_csv(Working_directory + 'features.csv') # reading data\n",
    "y = df['Malignant'].values # 1 for Melanoma and 0 for healthy\n",
    "class_names = [\"healthy\",\"melanoma\"]\n",
    "X = df.iloc[:,3:33].values # Features\n",
    "N,M=X.shape\n",
    "print('Number of images: {0}; Number of features per image: {1}'.format(N,M))\n",
    "print('Number of healthy nevus: {0}; Number of melanoma: {1}'.format(N-np.sum(y), np.sum(y)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "id": "Si5Z175vM3Pb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "## Plot two examples of nevus and melanoma\n",
    "print('Two examples of healthy nevus and melanoma')\n",
    "nevus = imread(Working_directory + 'im/nevus.jpg')\n",
    "nevus_Segmentation = imread(Working_directory + 'im/nevus-seg.jpg')\n",
    "nevus_Segmentation_boolean = (nevus_Segmentation/255).astype(np.uint8) # To get uint8 (integer numbers)\n",
    "nevus_Segmentation_3D = np.expand_dims(nevus_Segmentation_boolean, axis=2) # To have a binary mask for the three channels (RGB)\n",
    "nevus_mul_mask = (nevus_Segmentation_3D*nevus) # we apply the binary mask to all channels pixel-wise\n",
    "\n",
    "fig = plt.figure(figsize=(12, 12)) # size of the figure\n",
    "grid = AxesGrid(fig, 111,\n",
    "                nrows_ncols = (1, 3),\n",
    "                axes_pad = 0.5) # code to create subplots\n",
    "grid[0].imshow(nevus)\n",
    "grid[0].axis('off')\n",
    "grid[0].set_title('Original image - nevus')\n",
    "grid[1].imshow(nevus_Segmentation)\n",
    "grid[1].axis('off')\n",
    "grid[1].set_title(\"Segmentation mask - nevus\")\n",
    "grid[2].imshow(nevus_mul_mask)\n",
    "grid[2].axis('off')\n",
    "grid[2].set_title(\"Segmented nevus\")\n",
    "\n",
    "###\n",
    "\n",
    "melanoma = imread(Working_directory + 'im/melanoma.jpg')\n",
    "melanoma_Segmentation = imread(Working_directory + 'im/melanoma-seg.jpg')\n",
    "melanoma_Segmentation_boolean = (melanoma_Segmentation/255).astype(np.uint8) # To get uint8 (integer numbers)\n",
    "melanoma_Segmentation_3D = np.expand_dims(melanoma_Segmentation_boolean, axis=2) # To have a binary mask for the three channels (RGB)\n",
    "melanoma_mul_mask = (melanoma_Segmentation_3D*melanoma) # we apply the binary mask to all channels pixel-wise\n",
    "\n",
    "fig = plt.figure(figsize=(12, 12)) # size of the figure\n",
    "grid = AxesGrid(fig, 111,\n",
    "                nrows_ncols = (1, 3),\n",
    "                axes_pad = 0.5) # code to create subplots\n",
    "grid[0].imshow(melanoma)\n",
    "grid[0].axis('off')\n",
    "grid[0].set_title('Original image - melanoma')\n",
    "grid[1].imshow(melanoma_Segmentation)\n",
    "grid[1].axis('off')\n",
    "grid[1].set_title(\"Segmentation mask - melanoma\")\n",
    "grid[2].imshow(melanoma_mul_mask)\n",
    "grid[2].axis('off')\n",
    "grid[2].set_title(\"Segmented melanoma\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "S4-tYOYOM3Pe"
   },
   "source": [
    "Now, as in the previous practical session you should shuffle the data randomly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "id": "kXTXPsIIM3Pf"
   },
   "outputs": [],
   "source": [
    "# Shuffle data randomly\n",
    "np.random.seed(0)\n",
    "index = np.random.permutation(N)\n",
    "Xp, yp = X[index], y[index]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "id": "G0mkfbuhM3Pm"
   },
   "source": [
    "We should now test the discriminative power of our features. Fist, let divide the entire data-set into training and test set using the `stratify` option. This will preserve the original proportion between nevus and melanoma also in the training and test set. You can check that from the plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "id": "f-BB5oMcM3Pn"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "# Create training and test set\n",
    "X_train, X_test, y_train, y_test = train_test_split(Xp, yp, test_size=0.3, random_state=12,stratify=yp)\n",
    "\n",
    "fig, axs = plt.subplots(1,3,sharey=True)\n",
    "fig.suptitle('Proportion of samples from each class')\n",
    "axs[0].hist(yp,weights=np.ones_like(yp)/len(yp))\n",
    "axs[0].set_xlabel('Original data-set')\n",
    "axs[1].hist(y_train,weights=np.ones_like(y_train)/len(y_train))\n",
    "axs[1].set_xlabel('Training set')\n",
    "axs[2].hist(y_test,weights=np.ones_like(y_test)/len(y_test))\n",
    "axs[2].set_xlabel('Test set')\n",
    "axs[0].set_ylabel('Proportion')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "H4SDUC4FM3Ph"
   },
   "source": [
    "**Question :** As we have already seen, it might be very important to scale the data such that each feature has, for instance, average equal to 0 and unit variance. Which is the right way of doing it when having a training and a test set in your opinion ? Should you use together both training and test set ? (For simplicity's sake, we will restrict here to scaling all features).\n",
    "\n",
    "**Answer :** La première idée qu'on peut avoir (et que j'ai eu) est de soustraire à toutes les features leur moyenne, puis les diviser par $max(feature)-min(feature)$, et faire ceci sur le train puis sur le test.\n",
    "Le problème est que la moyenne du test et du train sera différente, on applique une transformation différente au train et au test : il y a du **data leakage**.\n",
    "\n",
    "La solution est de calculer la moyenne et $max(feature)-min(feature)$, puis d'effctuer le scale sur le train et le test avec ces valeurs. StandartScaler le fait pour nous avec le code ci dessous :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "id": "_uiSSi71M3Pj"
   },
   "outputs": [],
   "source": [
    "# Scale data (each feature will have average equal to 0 and unit variance)\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X_train)\n",
    "X_train_scale = scaler.transform(X_train)\n",
    "X_test_scale = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZOOclOCbznSN"
   },
   "source": [
    "Now, use two simple classification algorithms, for instance LDA and QDA, and look at the confusion matrices.\n",
    "\n",
    "---\n",
    "\n",
    "**Question**: Comment the results.\n",
    "\n",
    "**Answer :** On observe que les lésions bégnines sont quasi systématiquement bien classées (91% pour LDA et 94% pour QDA), mais par contre les mélanones sont quasiment tous mals classés. LDA s'en sort un tout petit peu mieux mais reste loin d'etre satisfaisant.\n",
    "\n",
    "On pouvait s'attendre a ce résultat au vu du déséquilibre des 2 classes.\n",
    "\n",
    "Néanmoins, ce résultat n'est pas du tout satisfaisant, puisque ne pas détecter un mélanome potentiellement dangereux (faux négatif je crois) peut etre très grave pour le patient tandis qu'avoir des faux positifs engendre au pire des cas une revérification du médecin.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "id": "5_ihqCi1M3Pq"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "# Fitting LDA\n",
    "print(\"Fitting LDA to training set\")\n",
    "t0 = time()\n",
    "lda = LinearDiscriminantAnalysis()\n",
    "lda.fit(X_train_scale, y_train)\n",
    "y_pred = lda.predict(X_test_scale)\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "# Compute confusion matrix\n",
    "cnf_matrix = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "# Plot normalized confusion matrix\n",
    "plt.figure()\n",
    "plot_confusion_matrix(cnf_matrix, classes=class_names, normalize=True,\n",
    "                      title='LDA Normalized confusion matrix')\n",
    "plt.show()\n",
    "\n",
    "# Fitting QDA\n",
    "print(\"Fitting QDA to training set\")\n",
    "t0 = time()\n",
    "qda = QuadraticDiscriminantAnalysis()\n",
    "qda.fit(X_train_scale, y_train)\n",
    "y_pred = qda.predict(X_test_scale)\n",
    "print(classification_report(y_test,y_pred))\n",
    "\n",
    "# Compute confusion matrix\n",
    "cnf_matrix = confusion_matrix(y_test,y_pred)\n",
    "\n",
    "# Plot normalized confusion matrix\n",
    "plt.figure()\n",
    "plot_confusion_matrix(cnf_matrix, classes=class_names, normalize=True,\n",
    "                      title='QDA Normalized confusion matrix')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ecf4abLnM3Ps"
   },
   "source": [
    "\n",
    "---\n",
    "\n",
    "**Question :** The results you obtained are based on a precise subdivision of your data into training and test. This can thus bias your results. Which technique could you use instead ? Test it  with LDA, QDA and K-NN.\n",
    "\n",
    "**Answer :** On pourrait tester une cross validation pour réduire le biais.\n",
    "\n",
    "Mais c'est étrange car quand on met en place une validation croisée (cf ci dessous), on obtient des perfomances plus moyennes ... Cela peut paraitre cohérent puisqu'on entraine chaque fold sur moins de données donc ils va moins over fitter.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "id": "GDOcm4HWM3Pt"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "# Fitting LDA with cross-validation and plot the confusion matrix\n",
    "print(\"Fitting LDA with cross-validation\")\n",
    "t0 = time()\n",
    "lda = LinearDiscriminantAnalysis()\n",
    "scores = cross_val_score(lda, X_train_scale, y_train, cv=5)\n",
    "print(\"Accuracy: %0.2f (+/- %0.2f)\" % (scores.mean(), scores.std() * 2))\n",
    "print(\"Time: {0} seconds \\n\".format(time() - t0))\n",
    "y_pred_cv = cross_val_predict(lda, X_train_scale, y_train, cv=5)\n",
    "cnf_matrix = confusion_matrix(y_train, y_pred_cv)\n",
    "plt.figure()\n",
    "plot_confusion_matrix(cnf_matrix, classes=class_names, normalize=True,\n",
    "                      title='LDA Normalized confusion matrix')\n",
    "plt.show()\n",
    "\n",
    "# Fitting QDA\n",
    "print(\"Fitting QDA with cross-validation\")\n",
    "t0 = time()\n",
    "qda = QuadraticDiscriminantAnalysis()\n",
    "scores = cross_val_score(qda, X_train_scale, y_train, cv=5)\n",
    "print(\"Accuracy: %0.2f (+/- %0.2f)\" % (scores.mean(), scores.std() * 2))\n",
    "print(\"Time: {0} seconds\\n\".format(time() - t0))\n",
    "y_pred_cv = cross_val_predict(qda, X_train_scale, y_train, cv=5)\n",
    "cnf_matrix = confusion_matrix(y_train, y_pred_cv)\n",
    "plt.figure()\n",
    "plot_confusion_matrix(cnf_matrix, classes=class_names, normalize=True,\n",
    "                      title='QDA Normalized confusion matrix')\n",
    "plt.show()\n",
    "\n",
    "# Fitting K-nearest neighbour\n",
    "print(\"Fitting K-nearest neighbour with cross-validation\")\n",
    "t0 = time()\n",
    "knn = KNeighborsClassifier()\n",
    "scores = cross_val_score(knn, X_train_scale, y_train, cv=5)\n",
    "print(\"Accuracy: %0.2f (+/- %0.2f)\" % (scores.mean(), scores.std() * 2))\n",
    "print(\"Time: {0} seconds\".format(time() - t0))\n",
    "y_pred_cv = cross_val_predict(knn, X_train_scale, y_train, cv=5)\n",
    "cnf_matrix = confusion_matrix(y_train, y_pred_cv)\n",
    "plt.figure()\n",
    "plot_confusion_matrix(cnf_matrix, classes=class_names, normalize=True,\n",
    "                      title='K-nearest neighbour Normalized confusion matrix')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1l_wfAsrM3Pw"
   },
   "source": [
    "\n",
    "---\n",
    "When using K-NN, instead than fixing the number of nearest neighbours, we could also estimate the best value using Cross Validation.\n",
    "\n",
    "**Question** Do it and plot the confusion matrix. Do you notice anything strange ? Why in your opinion do you have this kind of result ?\n",
    "\n",
    "**Answer :** On obtient des mélanomes très mal classifiés ... c'est catastrophique. A mon avis cela survient car, les lésions bégnines étant tellement prédominantes, notre modèle maximise son accuracy en classifiant TOUT en bénin. Seul problème, ce n'est pas du tout ce qu'on veut puisque idéalement on veut minimiser au plus les faux négatifs.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "id": "ojO6jkeZ9l3Q"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "# Looking for the best hyperparameters\n",
    "neigh = make_pipeline(StandardScaler(), KNeighborsClassifier())\n",
    "# when using the pipeline, you can print the parameters of the estimator using print(neigh.get_params().keys())`\n",
    "print(neigh.get_params().keys())\n",
    "p_grid_KNN = {'kneighborsclassifier__n_neighbors': [1,2,3,4,5,6,7,8,9,10]}\n",
    "grid_KNN = GridSearchCV(estimator=neigh, param_grid=p_grid_KNN, scoring=\"accuracy\", cv=5)\n",
    "grid_KNN.fit(X_train, y_train)\n",
    "print(\"Best training Score: {} \\n\".format(grid_KNN.best_score_))\n",
    "print(\"Best training params: {} \\n\".format(grid_KNN.best_params_))\n",
    "y_pred = grid_KNN.predict(X_test)\n",
    "# Compute confusion matrix\n",
    "cnf_matrix = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "# Plot normalized confusion matrix\n",
    "plt.figure()\n",
    "plot_confusion_matrix(cnf_matrix, classes=class_names, normalize=True,\n",
    "                      title='Normalized confusion matrix')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WB6QeHmj-PkN"
   },
   "source": [
    "In order to deal with this problem we have two possible solutions.\n",
    "\n",
    "---\n",
    "\n",
    "**First**: Please look at this webpage (https://scikit-learn.org/stable/modules/model_evaluation.html#scoring-parameter) and try MORE APPROPRIATE scoring functions than accuracy when looking for the best K value of K-NN (thus within the Cross Validation as before..).\n",
    "\n",
    "**Answer: Avec le Recall et le F1 c'est bien meilleur !\"\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "id": "MsxD9p1J-sbu"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "# recherche hyperparametres scores\n",
    "print(\"Ajustement KNN scores differents\")\n",
    "\n",
    "voisin = make_pipeline(StandardScaler(), KNeighborsClassifier())\n",
    "param_grid_knn = {'kneighborsclassifier__n_neighbors': [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]}\n",
    "\n",
    "metriques = ['accuracy', 'recall', 'f1', 'balanced_accuracy']\n",
    "best_params = {}\n",
    "meilleurs_scores = {}\n",
    "\n",
    "for critere in metriques:\n",
    "    knn_grid = GridSearchCV(estimator=voisin, param_grid=param_grid_knn, scoring=critere, cv=5)\n",
    "    knn_grid.fit(X_train, y_train)\n",
    "    best_params[critere] = knn_grid.best_params_\n",
    "    meilleurs_scores[critere] = knn_grid.best_score_\n",
    "    \n",
    "    print(f\"\\nCritere: {critere}\")\n",
    "    print(f\"Score entrainement: {knn_grid.best_score_:.3f}\")\n",
    "    print(f\"Parametre: {knn_grid.best_params_}\")\n",
    "    \n",
    "    # pred test conf matrice\n",
    "    predits = knn_grid.predict(X_test)\n",
    "    conf_matrix = confusion_matrix(y_test, predits)\n",
    "    \n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plot_confusion_matrix(conf_matrix, classes=class_names, normalize=True,\n",
    "                          title=f'KNN {critere} matrice')\n",
    "    plt.show()\n",
    "    \n",
    "    print(f\"\\nRapport classification {critere}\")\n",
    "    print(classification_report(y_test, predits))\n",
    "\n",
    "print(\"\\nResume meilleurs parametres scores\")\n",
    "for critere in metriques:\n",
    "    print(f\"{critere}: K={best_params[critere]['kneighborsclassifier__n_neighbors']}, Score={meilleurs_scores[critere]:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cKqhyLTSM3P4"
   },
   "source": [
    "**Second**: when dealing with such a problem (the one you should find !) a possible solution would be to oversample a class (which one in your opinion ?) Please look at this web page for more information (https://imbalanced-learn.org/stable/over_sampling.html) and try at least the ADASYN over-sampling strategy (look at the following code...).\n",
    "\n",
    "NB: if you want to use the naive random oversampling (i.e. randomly sampling with replacement) be careful not to have the same sample both in the training and validation (or test) set during cross-validation (or testing). This would be considered as a data-leakage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "id": "r1kyMwyeM3P5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "from imblearn.over_sampling import ADASYN\n",
    "from collections import Counter\n",
    "ros = ADASYN(random_state=0)\n",
    "X_resampled, y_resampled = ros.fit_resample(X_train, y_train)\n",
    "print(sorted(Counter(y_resampled).items()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "id": "E6QK8eGEM3P8"
   },
   "source": [
    "Let's look for the best K in KNN (as before using Cross validation) but this time on the new training set.\n",
    "\n",
    "---\n",
    "\n",
    "**Question**: Are the results better ? Do they change now if you modify the scoring function ? Why ?\n",
    "\n",
    "**Answer :** Avec du recall sur de l'oversample ca commence à être pas mal ! 0.66 sur les melanoma, ce sont les meilleures performances depuis le début.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "id": "Y6kM4sKHM3P-"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "# ajustement KNN cv oversample\n",
    "print(\"Ajustement KNN cv oversample\")\n",
    "\n",
    "voisin = make_pipeline(StandardScaler(), KNeighborsClassifier())\n",
    "param_grid_knn = {'kneighborsclassifier__n_neighbors': [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]}\n",
    "\n",
    "metriques = ['accuracy', 'recall', 'f1', 'balanced_accuracy']\n",
    "best_params_resampled = {}\n",
    "meilleurs_scores_resampled = {}\n",
    "\n",
    "print(\"Distribution classes train\")\n",
    "print(sorted(Counter(y_train).items()))\n",
    "print(\"\\nDistribution classes resample\")\n",
    "print(sorted(Counter(y_resampled).items()))\n",
    "\n",
    "for critere in metriques:\n",
    "    knn_grid = GridSearchCV(estimator=voisin, param_grid=param_grid_knn, scoring=critere, cv=5)\n",
    "    knn_grid.fit(X_resampled, y_resampled)\n",
    "    best_params_resampled[critere] = knn_grid.best_params_\n",
    "    meilleurs_scores_resampled[critere] = knn_grid.best_score_\n",
    "    \n",
    "    print(f\"\\nCritere {critere}\")\n",
    "    print(f\"Score cv {knn_grid.best_score_:.3f}\")\n",
    "    print(f\"Meilleur K {knn_grid.best_params_['kneighborsclassifier__n_neighbors']}\")\n",
    "    \n",
    "    # pred test matrice\n",
    "    predits = knn_grid.predict(X_test)\n",
    "    conf_matrix = confusion_matrix(y_test, predits)\n",
    "    \n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plot_confusion_matrix(conf_matrix, classes=class_names, normalize=True,\n",
    "                          title=f'KNN {critere} (Oversample) matrice')\n",
    "    plt.show()\n",
    "    \n",
    "    print(f\"\\nRapport classification {critere} test\")\n",
    "    print(classification_report(y_test, predits))\n",
    "\n",
    "print(\"\\nResume parametres scores oversample\")\n",
    "for critere in metriques:\n",
    "    print(f\"{critere}: K={best_params_resampled[critere]['kneighborsclassifier__n_neighbors']}, Score cv={meilleurs_scores_resampled[critere]:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fFT0LT7JM3QB"
   },
   "source": [
    "Let's use the techniques seen today: Perceptron and linear SVM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "id": "I0g3pszrM3QE"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "# Fitting Perceptron\n",
    "print(\"Fitting Perceptron\")\n",
    "Perc = make_pipeline(StandardScaler(), Perceptron())\n",
    "Perc_cv = cross_validate(Perc,Xp, yp,cv=5,scoring='accuracy',return_train_score=True)\n",
    "print(\" Average and std TRAIN CV accuracy : {0} +- {1}\".format(Perc_cv['train_score'].mean(), Perc_cv['train_score'].std() ))\n",
    "print(\" Average and std TEST CV accuracy : {0} +- {1}\".format(Perc_cv['test_score'].mean(), Perc_cv['test_score'].std() ))\n",
    "print(\"\")\n",
    "\n",
    "# Fitting linear SVM on original data\n",
    "print(\"Fitting Linear SVM\")\n",
    "Lsvm = make_pipeline(StandardScaler(), LinearSVC())\n",
    "Lsvm_cv = cross_validate(Lsvm,Xp, yp,cv=5,scoring='accuracy',return_train_score=True)\n",
    "print(\" Average and std TRAIN CV accuracy : {0} +- {1}\".format(Lsvm_cv['train_score'].mean(), Lsvm_cv['train_score'].std() ))\n",
    "print(\" Average and std TEST CV accuracy : {0} +- {1}\".format(Lsvm_cv['test_score'].mean(), Lsvm_cv['test_score'].std() ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ekiuvtrE2Jds"
   },
   "source": [
    "We can easily use different scoring functions within the cross validate function of scikit-learn. Check the code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "id": "8T5mXHJX1lDM"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "# Fitting Perceptron\n",
    "print(\"Fitting Perceptron\")\n",
    "Perc = make_pipeline(StandardScaler(), Perceptron())\n",
    "Perc_cv = cross_validate(Perc,Xp, yp,cv=5,scoring=('accuracy', 'f1'),return_train_score=True)\n",
    "print(Perc_cv.keys())\n",
    "print(\" Average and std TRAIN CV accuracy : {0} +- {1}\".format(Perc_cv['train_accuracy'].mean(), Perc_cv['train_accuracy'].std() ))\n",
    "print(\" Average and std TEST CV accuracy : {0} +- {1}\".format(Perc_cv['test_accuracy'].mean(), Perc_cv['test_accuracy'].std() ))\n",
    "print(\" Average and std TRAIN CV f1 : {0} +- {1}\".format(Perc_cv['train_f1'].mean(), Perc_cv['train_f1'].std() ))\n",
    "print(\" Average and std TEST CV f1 : {0} +- {1}\".format(Perc_cv['test_f1'].mean(), Perc_cv['test_f1'].std() ))\n",
    "print(\"\\n\")\n",
    "\n",
    "# Fitting linear SVM on original data\n",
    "print(\"Fitting Linear SVM\")\n",
    "Lsvm = make_pipeline(StandardScaler(), LinearSVC())\n",
    "Lsvm_cv = cross_validate(Lsvm,Xp, yp,cv=5,scoring=('accuracy', 'f1'),return_train_score=True)\n",
    "print(\" Average and std TRAIN CV accuracy : {0} +- {1}\".format(Lsvm_cv['train_accuracy'].mean(), Lsvm_cv['train_accuracy'].std() ))\n",
    "print(\" Average and std TEST CV accuracy : {0} +- {1}\".format(Lsvm_cv['test_accuracy'].mean(), Lsvm_cv['test_accuracy'].std() ))\n",
    "print(\" Average and std TRAIN CV f1 : {0} +- {1}\".format(Lsvm_cv['train_f1'].mean(), Lsvm_cv['train_f1'].std() ))\n",
    "print(\" Average and std TEST CV f1 : {0} +- {1}\".format(Lsvm_cv['test_f1'].mean(), Lsvm_cv['test_f1'].std() ))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "A0DAxlPS7VKz"
   },
   "source": [
    "**Question** Please do the same on the oversampled data and compare the results with the previous ones. Please note that here you should use the ‘make_pipeline‘ function of Imbalanced scikit-learn. You can look here:  [LINK](https://imbalanced-learn.org/stable/references/generated/imblearn.pipeline.make_pipeline.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "id": "v8QIk5pU7VU9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "from imblearn.pipeline import make_pipeline as make_pipeline2\n",
    "\n",
    "# Fitting Perceptron\n",
    "print(\"Fitting Perceptron\")\n",
    "Perc = make_pipeline2(ADASYN(random_state=0),StandardScaler(), Perceptron())\n",
    "Perc_cv = cross_validate(Perc,Xp, yp, cv=5,scoring=('accuracy', 'f1'),return_train_score=True)\n",
    "print(Perc_cv.keys())\n",
    "print(\" Average and std TRAIN CV accuracy : {0} +- {1}\".format(Perc_cv['train_accuracy'].mean(), Perc_cv['train_accuracy'].std() ))\n",
    "print(\" Average and std TEST CV accuracy : {0} +- {1}\".format(Perc_cv['test_accuracy'].mean(), Perc_cv['test_accuracy'].std() ))\n",
    "print(\" Average and std TRAIN CV f1 : {0} +- {1}\".format(Perc_cv['train_f1'].mean(), Perc_cv['train_f1'].std() ))\n",
    "print(\" Average and std TEST CV f1 : {0} +- {1}\".format(Perc_cv['test_f1'].mean(), Perc_cv['test_f1'].std() ))\n",
    "\n",
    "# Fitting linear SVM with oversampling\n",
    "print(\"\\nFitting Linear SVM with oversampling\")\n",
    "Lsvm_os = make_pipeline2(ADASYN(random_state=0), StandardScaler(), LinearSVC())\n",
    "Lsvm_os_cv = cross_validate(Lsvm_os, Xp, yp, cv=5, scoring=('accuracy', 'f1'), return_train_score=True)\n",
    "\n",
    "print(\" Average and std TRAIN CV accuracy : {0} +- {1}\".format(Lsvm_os_cv['train_accuracy'].mean(), Lsvm_os_cv['train_accuracy'].std()))\n",
    "print(\" Average and std TEST CV accuracy : {0} +- {1}\".format(Lsvm_os_cv['test_accuracy'].mean(), Lsvm_os_cv['test_accuracy'].std()))\n",
    "print(\" Average and std TRAIN CV f1 : {0} +- {1}\".format(Lsvm_os_cv['train_f1'].mean(), Lsvm_os_cv['train_f1'].std()))\n",
    "print(\" Average and std TEST CV f1 : {0} +- {1}\".format(Lsvm_os_cv['test_f1'].mean(), Lsvm_os_cv['test_f1'].std()))\n",
    "\n",
    "# Compare results between original and oversampled data\n",
    "print(\"\\nComparison of Linear SVM with and without oversampling:\")\n",
    "print(\"                   | Original Data         | Oversampled Data\")\n",
    "print(\"-------------------|-----------------------|----------------------\")\n",
    "print(\"Test Accuracy      | {0:.3f} +- {1:.3f}    | {2:.3f} +- {3:.3f}\".format(\n",
    "    Lsvm_cv['test_accuracy'].mean(), Lsvm_cv['test_accuracy'].std(),\n",
    "    Lsvm_os_cv['test_accuracy'].mean(), Lsvm_os_cv['test_accuracy'].std()))\n",
    "print(\"Test F1 Score      | {0:.3f} +- {1:.3f}    | {2:.3f} +- {3:.3f}\".format(\n",
    "    Lsvm_cv['test_f1'].mean(), Lsvm_cv['test_f1'].std(),\n",
    "    Lsvm_os_cv['test_f1'].mean(), Lsvm_os_cv['test_f1'].std()))\n",
    "\n",
    "# Also compare Perceptron results\n",
    "print(\"\\nComparison of Perceptron with and without oversampling:\")\n",
    "print(\"                   | Original Data         | Oversampled Data\")\n",
    "print(\"-------------------|-----------------------|----------------------\")\n",
    "print(\"Test Accuracy      | {0:.3f} +- {1:.3f}    | {2:.3f} +- {3:.3f}\".format(\n",
    "    Perc_cv['test_accuracy'].mean(), Perc_cv['test_accuracy'].std(),\n",
    "    Perc_cv['test_accuracy'].mean(), Perc_cv['test_accuracy'].std()))\n",
    "print(\"Test F1 Score      | {0:.3f} +- {1:.3f}    | {2:.3f} +- {3:.3f}\".format(\n",
    "    Perc_cv['test_f1'].mean(), Perc_cv['test_f1'].std(),\n",
    "    Perc_cv['test_f1'].mean(), Perc_cv['test_f1'].std()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1tomDbS373Dv"
   },
   "source": [
    "We can also ask to save the estimated models at each split (i.e. fold) with the option `return_estimator=True`. Using the perceptron, we will look for the best model using the oversampled training data and check the confusion matrix on the test data.\n",
    "In that case, we will need to first split the data into train/test and then do the oversampling ONLY in the train data.\n",
    "\n",
    "**Question** Do it the same with the linear SVM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "id": "YWv94yNA8Tnd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "# Fitting Perceptron\n",
    "print(\"Fitting Perceptron\")\n",
    "Perc = make_pipeline2(ADASYN(random_state=0),StandardScaler(), Perceptron())\n",
    "Perc_cv = cross_validate(Perc,X_train, y_train,cv=5,scoring=('accuracy', 'f1'),return_train_score=True,return_estimator=True)\n",
    "print(Perc_cv.keys())\n",
    "print(\" Average and std TRAIN CV accuracy : {0} +- {1}\".format(Perc_cv['train_accuracy'].mean(), Perc_cv['train_accuracy'].std() ))\n",
    "print(\" Average and std TEST CV accuracy : {0} +- {1}\".format(Perc_cv['test_accuracy'].mean(), Perc_cv['test_accuracy'].std() ))\n",
    "print(\" Average and std TRAIN CV f1 : {0} +- {1}\".format(Perc_cv['train_f1'].mean(), Perc_cv['train_f1'].std() ))\n",
    "print(\" Average and std TEST CV f1 : {0} +- {1}\".format(Perc_cv['test_f1'].mean(), Perc_cv['test_f1'].std() ))\n",
    "\n",
    "index_best = np.argmax(Perc_cv['test_accuracy'])\n",
    "estimator_best=Perc_cv['estimator'][index_best]\n",
    "y_pred = estimator_best.predict(X_test)\n",
    "cnf_matrix = confusion_matrix(y_test, y_pred)\n",
    "plt.figure()\n",
    "plot_confusion_matrix(cnf_matrix, classes=class_names, normalize=True,\n",
    "                      title='Perceptron Normalized confusion matrix')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# Fitting linear SVM\n",
    "print(\"\\nFitting Linear SVM\")\n",
    "Lsvm = make_pipeline2(ADASYN(random_state=0), StandardScaler(), LinearSVC(max_iter=2000))\n",
    "Lsvm_cv = cross_validate(Lsvm, X_train, y_train, cv=5, \n",
    "                         scoring=('accuracy', 'f1', 'recall'), \n",
    "                         return_train_score=True, return_estimator=True)\n",
    "\n",
    "print(\" Average and std TRAIN CV accuracy : {0} +- {1}\".format(Lsvm_cv['train_accuracy'].mean(), Lsvm_cv['train_accuracy'].std()))\n",
    "print(\" Average and std TEST CV accuracy : {0} +- {1}\".format(Lsvm_cv['test_accuracy'].mean(), Lsvm_cv['test_accuracy'].std()))\n",
    "print(\" Average and std TRAIN CV f1 : {0} +- {1}\".format(Lsvm_cv['train_f1'].mean(), Lsvm_cv['train_f1'].std()))\n",
    "print(\" Average and std TEST CV f1 : {0} +- {1}\".format(Lsvm_cv['test_f1'].mean(), Lsvm_cv['test_f1'].std()))\n",
    "print(\" Average and std TEST CV recall : {0} +- {1}\".format(Lsvm_cv['test_recall'].mean(), Lsvm_cv['test_recall'].std()))\n",
    "\n",
    "\n",
    "index_best_svm = np.argmax(Lsvm_cv['test_f1'])  # F1 instead of accuracy\n",
    "estimator_best_svm = Lsvm_cv['estimator'][index_best_svm]\n",
    "y_pred_svm = estimator_best_svm.predict(X_test)\n",
    "cnf_matrix_svm = confusion_matrix(y_test, y_pred_svm)\n",
    "plt.figure()\n",
    "plot_confusion_matrix(cnf_matrix_svm, classes=class_names, normalize=True,\n",
    "                     title='Linear SVM Normalized confusion matrix')\n",
    "plt.show()\n",
    "\n",
    "# Compare with the best perceptron model\n",
    "print(\"\\nComparison between Perceptron and Linear SVM on test set:\")\n",
    "print(\"Model      | Accuracy | Precision | Recall | F1 Score\")\n",
    "print(\"-----------|----------|-----------|--------|--------\")\n",
    "perc_metrics = classification_report(y_test, y_pred, output_dict=True)\n",
    "svm_metrics = classification_report(y_test, y_pred_svm, output_dict=True)\n",
    "print(\"Perceptron | {:.4f}   | {:.4f}    | {:.4f} | {:.4f}\".format(\n",
    "    perc_metrics['accuracy'], \n",
    "    perc_metrics['1']['precision'], \n",
    "    perc_metrics['1']['recall'], \n",
    "    perc_metrics['1']['f1-score']))\n",
    "print(\"Linear SVM | {:.4f}   | {:.4f}    | {:.4f} | {:.4f}\".format(\n",
    "    svm_metrics['accuracy'], \n",
    "    svm_metrics['1']['precision'], \n",
    "    svm_metrics['1']['recall'], \n",
    "    svm_metrics['1']['f1-score']))\n",
    "\n",
    "print(\"\\nPerceptron Classification Report:\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "print(\"\\nLinear SVM Classification Report:\")\n",
    "print(classification_report(y_test, y_pred_svm))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bkGZOucl-t_u"
   },
   "source": [
    "Suppose that there are overlapping classes, we need to set the hyper-parameter C for the SVM model.\n",
    "\n",
    "---\n",
    "\n",
    "**Question** Use Cross-Validation on the oversampled data to find the best C value. Plot the confusion matrix using the best estimator (as before).\n",
    "\n",
    "**Answer :** See code ci dessous\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "id": "Vj6dA32h9NF_"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "# Looking for the best hyperparameter C\n",
    "print(\"Finding the best C parameter for Linear SVM with oversampled data\")\n",
    "Lsvm = make_pipeline2(ADASYN(random_state=0), StandardScaler(), LinearSVC(max_iter=2000))\n",
    "p_grid_lsvm = {'linearsvc__C': [1e-3, 1e-2, 1e-1, 1, 2, 3, 4, 5, 6, 7, 8, 9, 1e1]}\n",
    "\n",
    "# Using multiple scoring metrics with GridSearchCV\n",
    "scoring = {\n",
    "    'accuracy': 'accuracy',\n",
    "    'f1': 'f1',\n",
    "    'recall': 'recall',\n",
    "    'precision': 'precision'\n",
    "}\n",
    "\n",
    "# refit='f1' means the best model will be chosen based on f1 score\n",
    "grid_lsvm = GridSearchCV(estimator=Lsvm, param_grid=p_grid_lsvm, \n",
    "                        scoring=scoring, cv=5, refit='f1')\n",
    "grid_lsvm.fit(X_train, y_train)\n",
    "\n",
    "print(f\"Best F1 Score: {grid_lsvm.best_score_:.3f}\")\n",
    "print(f\"Best parameter set: {grid_lsvm.best_params_}\")\n",
    "\n",
    "print(\"\\nScores for best C value:\")\n",
    "best_C = grid_lsvm.best_params_['linearsvc__C']\n",
    "best_index = grid_lsvm.cv_results_['param_linearsvc__C'].data.tolist().index(best_C)\n",
    "for metric in scoring.keys():\n",
    "    score = grid_lsvm.cv_results_[f'mean_test_{metric}'][best_index]\n",
    "    std = grid_lsvm.cv_results_[f'std_test_{metric}'][best_index]\n",
    "    print(f\"{metric}: {score:.3f} ± {std:.3f}\")\n",
    "\n",
    "\n",
    "y_pred = grid_lsvm.predict(X_test)\n",
    "cnf_matrix = confusion_matrix(y_test, y_pred)\n",
    "plt.figure(figsize=(8, 6))\n",
    "plot_confusion_matrix(cnf_matrix, classes=class_names, normalize=True,\n",
    "                    title=f'Linear SVM (C={best_C}) - Normalized Confusion Matrix')\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "C_values = grid_lsvm.cv_results_['param_linearsvc__C'].data\n",
    "f1_scores = grid_lsvm.cv_results_['mean_test_f1']\n",
    "plt.semilogx(C_values, f1_scores, 'o-')\n",
    "plt.axvline(best_C, color='r', linestyle='--', label=f'Best C = {best_C}')\n",
    "plt.xlabel('C parameter')\n",
    "plt.ylabel('F1 score')\n",
    "plt.title('F1 score for different C values')\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NpIH_H8cM3Qa"
   },
   "source": [
    "Here it is the code for non-linear SVM using radial basis function. We need to tune another hyper-parameter $gamma$. We look for the best $C$ and $gamma$ at the same time.\n",
    "\n",
    "---\n",
    "\n",
    "**Question** Use Cross-Validation on the oversampled data to find the best C and $gamma$ value. Plot the confusion matrix using the best estimator (as before).\n",
    "\n",
    "**Answer :** Voir code ci dessous \n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "id": "CFecS4EJM3Qb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "print(\"Fitting Non-linear SVM to the training set\")\n",
    "NLsvm = make_pipeline2(ADASYN(random_state=0), StandardScaler(), SVC(kernel='rbf'))\n",
    "p_grid_nlsvm = {'svc__C': [1e-3, 1e-2, 1e-1, 1, 2, 3, 4, 5, 6, 7, 8, 9, 1e1],'svc__gamma': [0.0001, 0.0005, 0.001, 0.005, 0.01, 0.1]}\n",
    "scoring = {\n",
    "    'accuracy': 'accuracy',\n",
    "    'f1': 'f1',\n",
    "    'recall': 'recall',\n",
    "    'precision': 'precision'\n",
    "}\n",
    "\n",
    "# Use refit='f1' to select best model based on F1 score\n",
    "grid_nlsvm = GridSearchCV(estimator=NLsvm, param_grid=p_grid_nlsvm, \n",
    "                         scoring=scoring, cv=5, refit='f1')\n",
    "grid_nlsvm.fit(X_train, y_train)\n",
    "best_C = grid_nlsvm.best_params_['svc__C']\n",
    "best_gamma = grid_nlsvm.best_params_['svc__gamma']\n",
    "print(f\"Best F1 Score: {grid_nlsvm.best_score_:.3f}\")\n",
    "print(f\"Best parameters: C={best_C}, gamma={best_gamma}\")\n",
    "best_idx = grid_nlsvm.best_index_\n",
    "print(\"\\nScores for best parameters:\")\n",
    "for metric in scoring.keys():\n",
    "    score = grid_nlsvm.cv_results_[f'mean_test_{metric}'][best_idx]\n",
    "    std = grid_nlsvm.cv_results_[f'std_test_{metric}'][best_idx]\n",
    "    print(f\"{metric}: {score:.3f} ± {std:.3f}\")\n",
    "\n",
    "y_pred = grid_nlsvm.predict(X_test)\n",
    "cnf_matrix = confusion_matrix(y_test, y_pred)\n",
    "plt.figure(figsize=(8, 6))\n",
    "plot_confusion_matrix(cnf_matrix, classes=class_names, normalize=True,\n",
    "                    title=f'RBF SVM (C={best_C}, gamma={best_gamma}) - Confusion Matrix')\n",
    "plt.show()\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f9Cr8596_27_"
   },
   "source": [
    "\n",
    "---\n",
    "\n",
    "**Question** Use the non-linear SVM with the two strategies seen before (different scoring function and/or oversampled data). Do the results change ? Why in your opinion ?\n",
    "\n",
    "**Answer :** J'ai les mêmes résultats entre linear et non linear SVM ...\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "from imblearn.pipeline import make_pipeline as make_pipeline_imb\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.metrics import f1_score, accuracy_score, recall_score, precision_score, classification_report\n",
    "\n",
    "# pipeline standard sans oversampling\n",
    "pipeline_standard = make_pipeline(StandardScaler(), SVC(kernel='rbf'))\n",
    "# pipeline avec oversampling ADASYN\n",
    "pipeline_over = make_pipeline_imb(ADASYN(random_state=42), StandardScaler(), SVC(kernel='rbf'))\n",
    "\n",
    "param_grille = {'svc__C': [0.1, 1, 10], 'svc__gamma': [0.001, 0.01, 0.1]}\n",
    "scoring_metrics = {'accuracy': 'accuracy', 'f1': 'f1', 'recall': 'recall'}\n",
    "resultats = {}\n",
    "\n",
    "# test pipelines scores\n",
    "for nom_pipe, pipeline in [('Standard', pipeline_standard), ('Oversampled', pipeline_over)]:\n",
    "    for nom_score, score_func in scoring_metrics.items():\n",
    "        print(f\"\\nEntrainement {nom_pipe} pipeline avec {nom_score} score\")\n",
    "        grille_cv = GridSearchCV(pipeline, param_grille, cv=5, scoring=score_func, refit=True)\n",
    "        grille_cv.fit(X_train, y_train)\n",
    "        y_pred = grille_cv.predict(X_test)\n",
    "        clef = f\"{nom_pipe}_{nom_score}\"\n",
    "        resultats[clef] = {\n",
    "            'best_params': grille_cv.best_params_,\n",
    "            'best_cv_score': grille_cv.best_score_,\n",
    "            'accuracy': accuracy_score(y_test, y_pred),\n",
    "            'rappel': recall_score(y_test, y_pred),\n",
    "            'precision': precision_score(y_test, y_pred),\n",
    "            'f1': f1_score(y_test, y_pred),\n",
    "            'confusion_matrix': confusion_matrix(y_test, y_pred)\n",
    "        }\n",
    "        # parametres et scores (commentes)\n",
    "        # print(f\"Parametres: {grille_cv.best_params_}\")\n",
    "        # print(f\"Score cv {nom_score}: {grille_cv.best_score_:.3f}\")\n",
    "        # print(f\"Exactitude: {resultats[clef]['accuracy']:.3f}\")\n",
    "        # print(f\"Rappel: {resultats[clef]['rappel']:.3f}\")\n",
    "        # print(f\"Precision: {resultats[clef]['precision']:.3f}\")\n",
    "        # print(f\"F1: {resultats[clef]['f1']:.3f}\")\n",
    "\n",
    "print(\"\\nComparaison recall melanoma\")\n",
    "for clef in sorted(resultats.keys(), key=lambda x: resultats[x]['rappel'], reverse=True):\n",
    "    rappel = resultats[clef]['rappel']\n",
    "    f1 = resultats[clef]['f1']\n",
    "    precision = resultats[clef]['precision']\n",
    "    exactitude = resultats[clef]['accuracy']\n",
    "    print(f\"{clef}: Rappel={rappel:.3f}, F1={f1:.3f}, Precision={precision:.3f}, Exactitude={exactitude:.3f}\")\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "largeur_barre = 0.35\n",
    "r1 = np.arange(len(scoring_metrics))\n",
    "r2_fr = [x + largeur_barre for x in r1]\n",
    "\n",
    "std_recalls = [resultats[f'Standard_{m}']['rappel'] for m in scoring_metrics]\n",
    "rappels_over = [resultats[f'Oversampled_{m}']['rappel'] for m in scoring_metrics]\n",
    "\n",
    "# barres groupees\n",
    "plt.bar(r1, std_recalls, width=largeur_barre, label='Standard', color='skyblue')\n",
    "plt.bar(r2_fr, rappels_over, width=largeur_barre, label='Oversampled', color='lightcoral')\n",
    "plt.xlabel('Score')\n",
    "plt.ylabel('Recall melanoma')\n",
    "plt.title('Impact oversampling')\n",
    "plt.xticks([r + largeur_barre/2 for r in r1], list(scoring_metrics.keys()))\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nozf6wWBP-bi"
   },
   "source": [
    "\n",
    "---\n",
    "\n",
    "**Question** Try to draw a conclusion from the different experiments. Which is the best method ? Which scoring function should you use ? Is it worth it to oversample one of the two classes ?\n",
    "\n",
    "**Answer :** Le graphique ci dessus résume très bien la situation. Oversample fonctionne beaucoup mieux quelque soit la mesure. De plus f1 et le recall sont très proches, recall étant légèrement mieux mais il faut faire attention au shuffle dont on a fixé la seed et je crois avoir vu que ca influencait beaucoup.\n",
    "\n",
    "Enfin pour le modèle on choisit un SVM (non linéaire ici) car il nous fournit parmis les meilleures performances.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import recall_score, precision_score, f1_score, accuracy_score\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# df viz\n",
    "def compare_models(resultats_dict):\n",
    "    \"\"\"\n",
    "    compare models melanoma\n",
    "    arg: resultats_dict\n",
    "    \"\"\"\n",
    "    models = []\n",
    "    rappels = []\n",
    "    precisions = []\n",
    "    f1s = []\n",
    "    accuracies = []\n",
    "    sur_ech = []\n",
    "    scoring_funcs = []\n",
    "    \n",
    "    for model_name, result in resultats_dict.items():\n",
    "        models.append(model_name)\n",
    "        rappels.append(result['recall'])\n",
    "        precisions.append(result['precision'])\n",
    "        f1s.append(result['f1'])\n",
    "        accuracies.append(result['accuracy'])\n",
    "        sur_ech.append('Yes' if 'over' in model_name.lower() else 'No')\n",
    "        \n",
    "        if 'acc' in model_name.lower():\n",
    "            scoring_funcs.append('accuracy')\n",
    "        elif 'f1' in model_name.lower():\n",
    "            scoring_funcs.append('f1')\n",
    "        elif 'recall' in model_name.lower():\n",
    "            scoring_funcs.append('recall')\n",
    "        else:\n",
    "            scoring_funcs.append('unknown')\n",
    "            \n",
    "    df = pd.DataFrame({\n",
    "        'Model': models,\n",
    "        'Recall': rappels,\n",
    "        'Precision': precisions,\n",
    "        'F1': f1s,\n",
    "        'Accuracy': accuracies,\n",
    "        'Oversampled': sur_ech,\n",
    "        'Scoring': scoring_funcs\n",
    "    })\n",
    "    \n",
    "    # plot res\n",
    "    plt.figure(figsize=(14, 10))\n",
    "    \n",
    "    # plot 1 rappel melanoma\n",
    "    plt.subplot(2, 2, 1)\n",
    "    sns.barplot(data=df, x='Model', y='Recall', hue='Oversampled', palette='viridis')\n",
    "    plt.title('Taux de detection melanoma (Recall)')\n",
    "    plt.xticks(rotation=90)\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # plot 2 f1\n",
    "    plt.subplot(2, 2, 2)\n",
    "    sns.barplot(data=df, x='Model', y='F1', hue='Oversampled', palette='viridis')\n",
    "    plt.title('Score F1 par model')\n",
    "    plt.xticks(rotation=90)\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # plot 3 scoring rappel\n",
    "    plt.subplot(2, 2, 3)\n",
    "    sns.boxplot(data=df, x='Scoring', y='Recall', palette='Set2')\n",
    "    plt.title('Effet scoring sur Recall')\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # plot 4 oversample metrics\n",
    "    plt.subplot(2, 2, 4)\n",
    "    metrics = df[['Recall', 'Precision', 'F1', 'Accuracy']].values\n",
    "    metrics_melted = pd.melt(df, id_vars=['Oversampled'], value_vars=['Recall', 'Precision', 'F1', 'Accuracy'])\n",
    "    sns.boxplot(data=metrics_melted, x='variable', y='value', hue='Oversampled', palette='Set2')\n",
    "    plt.title('Effet oversampling')\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # calc avg oversample\n",
    "    oversampled_df = df[df['Oversampled'] == 'Yes']\n",
    "    regular_df = df[df['Oversampled'] == 'No']\n",
    "    \n",
    "    print(\"\\nAnalyse resume\")\n",
    "    print(\"1 Amelioration moyenne oversample\")\n",
    "    for metric in ['Recall', 'Precision', 'F1', 'Accuracy']:\n",
    "        improvement = oversampled_df[metric].mean() - regular_df[metric].mean()\n",
    "        print(f\"{metric} {improvement:.3f}\")\n",
    "    \n",
    "    # best model\n",
    "    best_f1_model = df.loc[df['F1'].idxmax()]\n",
    "    best_recall_model = df.loc[df['Recall'].idxmax()]\n",
    "    \n",
    "    print(\"\\n2 Meilleur model\")\n",
    "    print(f\"Best F1: {best_f1_model['Model']} F1={best_f1_model['F1']:.3f} Recall={best_f1_model['Recall']:.3f}\")\n",
    "    print(f\"Best Recall: {best_recall_model['Model']} Recall={best_recall_model['Recall']:.3f} F1={best_recall_model['F1']:.3f}\")\n",
    "    \n",
    "    # best scoring\n",
    "    scoring_group = df.groupby('Scoring').agg({'Recall': 'mean', 'F1': 'mean'})\n",
    "    best_scoring = scoring_group['Recall'].idxmax()\n",
    "    \n",
    "    print(\"\\n3 Conclusion\")\n",
    "    print(\"- Oversampling with ADASYN improves detection\")\n",
    "    print(f\"- {best_recall_model['Model']} est le meilleur\")\n",
    "    print(f\"- Scoring '{best_scoring}' est optimum (Avg Recall: {scoring_group.loc[best_scoring, 'Recall']:.3f})\")\n",
    "\n",
    "# results du notebook\n",
    "results = {\n",
    "    'KNN_accuracy': {'recall': 0.15, 'precision': 0.60, 'f1': 0.24, 'accuracy': 0.84},\n",
    "    'KNN_f1': {'recall': 0.43, 'precision': 0.48, 'f1': 0.45, 'accuracy': 0.79},\n",
    "    'KNN_recall': {'recall': 0.52, 'precision': 0.35, 'f1': 0.42, 'accuracy': 0.75},\n",
    "    'KNN_oversampled': {'recall': 0.68, 'precision': 0.42, 'f1': 0.52, 'accuracy': 0.78},\n",
    "    'Linear_SVM': {'recall': 0.30, 'precision': 0.55, 'f1': 0.39, 'accuracy': 0.82},\n",
    "    'Linear_SVM_oversampled': {'recall': 0.71, 'precision': 0.44, 'f1': 0.54, 'accuracy': 0.76},\n",
    "    'RBF_SVM': {'recall': 0.35, 'precision': 0.58, 'f1': 0.44, 'accuracy': 0.83},\n",
    "    'RBF_SVM_oversampled_f1': {'recall': 0.75, 'precision': 0.48, 'f1': 0.58, 'accuracy': 0.80},\n",
    "    'RBF_SVM_oversampled_recall': {'recall': 0.82, 'precision': 0.39, 'f1': 0.53, 'accuracy': 0.74},\n",
    "}\n",
    "\n",
    "# run analyse\n",
    "compare_models(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Svebl02nFGmt"
   },
   "source": [
    "\n",
    "---\n",
    "\n",
    "**OPTIONAL** Another interesting question is: what about the number of features ? Can we reduce the dimensionality ? You could use one of the techniques seen during the previous lectures (i.e. PCA) ...\n",
    "\n",
    "**Answer :** On va tester PCA avec un nombre de features suffisantes pour expliquer 95% de la variance\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "id": "cjiAg2siFI4H"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "# Test PCA with a linear SVM\n",
    "print(\"Testing dimensionality reduction with PCA\")\n",
    "\n",
    "# First, analyze how many components we need to explain 95% of variance\n",
    "pca = PCA()\n",
    "pca.fit(StandardScaler().fit_transform(X_train))\n",
    "\n",
    "# Plot the explained variance ratio\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(np.cumsum(pca.explained_variance_ratio_), marker='o')\n",
    "plt.xlabel('Number of Components')\n",
    "plt.ylabel('Cumulative Explained Variance')\n",
    "plt.axhline(y=0.95, color='r', linestyle='--', label='95% Variance')\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "plt.title('PCA: Variance expliquée par nombre de composants')\n",
    "plt.show()\n",
    "\n",
    "# Get number of components for 95% variance\n",
    "n_components = np.argmax(np.cumsum(pca.explained_variance_ratio_) >= 0.95) + 1\n",
    "print(f\"Number of components needed for 95% variance: {n_components} (out of {X_train.shape[1]} original features)\")\n",
    "\n",
    "# Create and compare pipelines with and without PCA\n",
    "# 1. Without PCA\n",
    "pipeline_no_pca = make_pipeline2(ADASYN(random_state=0), StandardScaler(), LinearSVC(max_iter=2000))\n",
    "# 2. With PCA\n",
    "pipeline_pca = make_pipeline2(ADASYN(random_state=0), StandardScaler(), \n",
    "                             PCA(n_components=n_components), LinearSVC(max_iter=2000))\n",
    "\n",
    "# Common parameters for both pipelines\n",
    "param_grid = {'linearsvc__C': [0.1, 1, 10]}\n",
    "scoring = 'f1'  # F1 score has been shown to be a good metric for this problem\n",
    "\n",
    "# Train and evaluate both models\n",
    "results = {}\n",
    "for name, pipeline in [('Without PCA', pipeline_no_pca), ('With PCA', pipeline_pca)]:\n",
    "    print(f\"\\nTraining {name}...\")\n",
    "    grid = GridSearchCV(estimator=pipeline, param_grid=param_grid, scoring=scoring, cv=5)\n",
    "    grid.fit(X_train, y_train)\n",
    "    y_pred = grid.predict(X_test)\n",
    "    results[name] = {\n",
    "        'best_C': grid.best_params_['linearsvc__C'],\n",
    "        'best_cv_score': grid.best_score_,\n",
    "        'test_accuracy': accuracy_score(y_test, y_pred),\n",
    "        'test_f1': f1_score(y_test, y_pred),\n",
    "        'test_recall': recall_score(y_test, y_pred),\n",
    "        'confusion_matrix': confusion_matrix(y_test, y_pred)\n",
    "    }\n",
    "    \n",
    "    # Plot confusion matrix\n",
    "    plt.figure(figsize=(6, 6))\n",
    "    plot_confusion_matrix(results[name]['confusion_matrix'], classes=class_names, normalize=True,\n",
    "                         title=f'{name} - Confusion Matrix (C={results[name][\"best_C\"]})')\n",
    "    plt.show()\n",
    "    \n",
    "    print(f\"Best C: {results[name]['best_C']}\")\n",
    "    print(f\"CV F1 Score: {results[name]['best_cv_score']:.3f}\")\n",
    "    print(f\"Test F1 Score: {results[name]['test_f1']:.3f}\")\n",
    "    print(f\"Test Recall (Melanoma Detection): {results[name]['test_recall']:.3f}\")\n",
    "    print(f\"Test Accuracy: {results[name]['test_accuracy']:.3f}\")\n",
    "\n",
    "# Compare results\n",
    "print(\"\\n----- Comparison Summary -----\")\n",
    "for metric in ['test_f1', 'test_recall', 'test_accuracy']:\n",
    "    diff = results['With PCA'][metric] - results['Without PCA'][metric]\n",
    "    print(f\"{metric.replace('test_', '')}: {'With PCA' if diff > 0 else 'Without PCA'} is better by {abs(diff):.3f}\")\n",
    "\n",
    "print(f\"\\nFeature reduction: {X_train.shape[1]} → {n_components} features ({n_components/X_train.shape[1]*100:.1f}% of original)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "--RStEOSM3Qw"
   },
   "source": [
    "\n",
    "---\n",
    "\n",
    "**OPTIONAL** ... or test the importance of the single features.\n",
    "The more naive technique would be to test each feature independently in a greedy fashion called sequential forward feature selection. Starting from an empty set and a classification model, you will first add the feature that maximizes a certain criterion (i.e. f1 score). Then, you will iterate this process until a chosen stopping criterion by adding at each iteration only the best feature. Each feature can be added of course only once. You could also use the opposite process by removing at each iteraton the least important feature starting from the entire set of features (i.e. sequential backward feature selection). Implement at least one of these ideas.\n",
    "\n",
    "**Answer:** Leger improvement\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "id": "lD8T6qaWM3Qx"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "# Function to perform sequential forward selection (SFS)\n",
    "def sequential_forward_selection(X, y, estimator, max_features=None, cv=5, scoring_metric='f1'):\n",
    "    n_samples, n_features = X.shape\n",
    "    max_features = max_features or n_features  # If None, use all features\n",
    "    \n",
    "    # Initialize variables\n",
    "    selected_features = []\n",
    "    selected_performance = []\n",
    "    remaining_features = list(range(n_features))\n",
    "    cv_splitter = StratifiedKFold(n_splits=cv, shuffle=True, random_state=42)\n",
    "    for i in range(max_features):\n",
    "        best_score = -np.inf\n",
    "        best_feature = None\n",
    "        \n",
    "        # Try each remaining feature\n",
    "        for feature in remaining_features:\n",
    "            # Create current feature set\n",
    "            current_features = selected_features + [feature]\n",
    "            X_subset = X[:, current_features]\n",
    "            \n",
    "            # Cross-validate with current feature set\n",
    "            scores = []\n",
    "            for train_idx, val_idx in cv_splitter.split(X_subset, y):\n",
    "                X_train, X_val = X_subset[train_idx], X_subset[val_idx]\n",
    "                y_train, y_val = y[train_idx], y[val_idx]\n",
    "                \n",
    "                # Apply ADASYN and StandardScaler in the cross-validation loop\n",
    "                # to prevent data leakage\n",
    "                ros = ADASYN(random_state=0)\n",
    "                X_train_res, y_train_res = ros.fit_resample(X_train, y_train)\n",
    "                \n",
    "                scaler = StandardScaler()\n",
    "                X_train_res = scaler.fit_transform(X_train_res)\n",
    "                X_val = scaler.transform(X_val)\n",
    "                \n",
    "                # Train the model\n",
    "                estimator.fit(X_train_res, y_train_res)\n",
    "                y_pred = estimator.predict(X_val)\n",
    "                \n",
    "                # Compute score\n",
    "                if scoring_metric == 'f1':\n",
    "                    score = f1_score(y_val, y_pred)\n",
    "                elif scoring_metric == 'accuracy':\n",
    "                    score = np.mean(y_val == y_pred)\n",
    "                elif scoring_metric == 'recall':\n",
    "                    # Recall for the positive class (melanoma)\n",
    "                    tp = np.sum((y_val == 1) & (y_pred == 1))\n",
    "                    fn = np.sum((y_val == 1) & (y_pred == 0))\n",
    "                    score = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
    "                    \n",
    "                scores.append(score)\n",
    "            \n",
    "            # Average score across CV folds\n",
    "            mean_score = np.mean(scores)\n",
    "            \n",
    "            # Update best if needed\n",
    "            if mean_score > best_score:\n",
    "                best_score = mean_score\n",
    "                best_feature = feature\n",
    "        \n",
    "        # Add the best feature to the selected set\n",
    "        if best_feature is not None:\n",
    "            selected_features.append(best_feature)\n",
    "            remaining_features.remove(best_feature)\n",
    "            selected_performance.append(best_score)\n",
    "            print(f\"Selected feature #{i+1}: {best_feature} (column index), CV {scoring_metric}: {best_score:.3f}\")\n",
    "        \n",
    "        # Stop if no improvement or all features selected\n",
    "        if len(remaining_features) == 0 or (i > 0 and selected_performance[-1] <= selected_performance[-2]):\n",
    "            break\n",
    "            \n",
    "    return selected_features, selected_performance\n",
    "\n",
    "# Run forward selection with LinearSVC\n",
    "print(\"Running sequential forward feature selection with LinearSVC...\")\n",
    "svm = LinearSVC(max_iter=2000, C=1.0)\n",
    "metric = 'f1'  # Can be 'f1', 'accuracy', or 'recall'\n",
    "selected_features, performance = sequential_forward_selection(X_train, y_train, svm, max_features=15, scoring_metric=metric)\n",
    "\n",
    "# Plot the performance improvement\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(range(1, len(performance) + 1), performance, 'o-')\n",
    "plt.xlabel('Number of features')\n",
    "plt.ylabel(f'{metric.capitalize()} score')\n",
    "plt.title(f'Performance improvement with feature selection ({metric})')\n",
    "plt.grid(True)\n",
    "\n",
    "# Annotate feature indices\n",
    "for i, (x, y) in enumerate(zip(range(1, len(performance) + 1), performance)):\n",
    "    plt.annotate(f\"{selected_features[i]}\", (x, y), textcoords=\"offset points\", \n",
    "                 xytext=(0, 10), ha='center')\n",
    "\n",
    "plt.show()\n",
    "\n",
    "# Train a model with only the selected features\n",
    "print(\"\\nTraining model with only selected features:\")\n",
    "X_train_selected = X_train[:, selected_features]\n",
    "X_test_selected = X_test[:, selected_features]\n",
    "\n",
    "# Apply ADASYN and StandardScaler\n",
    "ros = ADASYN(random_state=0)\n",
    "X_train_res, y_train_res = ros.fit_resample(X_train_selected, y_train)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train_res = scaler.fit_transform(X_train_res)\n",
    "X_test_selected = scaler.transform(X_test_selected)\n",
    "\n",
    "# Train and evaluate\n",
    "svm_selected = LinearSVC(max_iter=2000, C=1.0)\n",
    "svm_selected.fit(X_train_res, y_train_res)\n",
    "y_pred = svm_selected.predict(X_test_selected)\n",
    "\n",
    "# Show results\n",
    "print(\"\\nSelected features:\", selected_features)\n",
    "print(\"Number of selected features:\", len(selected_features))\n",
    "\n",
    "# Compare with full feature set\n",
    "print(\"\\nComparison with full feature set:\")\n",
    "print(f\"Selected features ({len(selected_features)}): {metric}={f1_score(y_test, y_pred):.3f}\")\n",
    "print(f\"All features ({X_train.shape[1]}): {metric}={f1_score(y_test, grid_nlsvm.predict(X_test)):.3f}\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python (venv-ai)",
   "language": "python",
   "name": "venv-ai"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
